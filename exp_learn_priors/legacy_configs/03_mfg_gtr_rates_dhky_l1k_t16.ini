## ############################################################
## Template config file for write_run_nntreevb_exps.py
## 
## Hyper-parameters and default values 
## ############################################################

## ############################################################
## nntreevb_write_run_exps.py configuration sections
## ############################################################

## [slurm] and [evaluation] sections will be removed in the 
## final config files for nntreevb.py

[slurm]
run_slurm = True
## if run_slurm=False: the script will launch the jobs locally
## elif run_slurm=True: it will launch the jobs on slurm

## ############################################################
## WARNING:
## In the case of run_slurm=False, the script will run
## several tasks locally in the background. Make sure that you
## have the necessary resources on your machine.
## It can crash the system.
## You can modify the grid of parameters to be evaluated here 
## to reduce the number of scenarios.
## ############################################################

## Account: UPDATE your slurm account information here
account = ctb-banire
#account = def-banire
mail_user = amine.m.remita@gmail.com

## SLURM parameters
## ################
# If gpus_per_node > 0, device in [settings] will be updated 
# to cuda
gpus_per_node = 0
cpus_per_task = 24
exec_time = 02:00:00
mem = 80000M
## For testing
#exec_time = 00:05:00
#mem = 8000M

[evaluation]
#run_jobs = False
run_jobs = True
## If run_jobs = False: the script generates the nntreevb
## config files but it won't launch the jobs.
## If run_jobs=True: it runs the jobs

##
output_eval = ../results/exp_learn_priors/

max_iter = 2000
## For testing
#max_iter = 500

nb_parallel = ${slurm:cpus_per_task}

# for nntreevb_sum_plot_exps.py
report_n_epochs = 2000

## Remark: values can span multiple lines, as long as they are
## indented deeper than the first line of the value.
## Configparser fetchs the value as string, after that it will
## be casted to python objects using json.loads

# I will use HKY model with different means of kappa values
# to simulate the data
# HKY model needs two params: kappa and frequencies
# For frequencies, I use dirichlet(10.) to force a bit 
# a uniform distribution over them.
# For branch lengths, I use exponential(10.)|True

# Parameters of Gamma distributions to sample kappa values:
# mean=0.25, var=0.01: alpha=6.25,  beta=25.0
# mean=0.5,  var=0.01: alpha=25.,   beta=50.
# mean=1.,   var=0.01: alpha=100.,  beta=100.
# mean=2.,   var=0.01: alpha=400.,  beta=200.
# mean=4.,   var=0.01: alpha=1600., beta=400.

# Original batch of evaluations
#evaluations = {
#    "nb_sites@data": ["1000", "5000"],
#    "nb_taxa@data": ["16", "32", "64"],
#    "sim_kappa@data": ["gamma(6.25,25.)|True", "gamma(25.,50.)|True", "gamma(100.,100.)|True", "gamma(400.,200.)|True", "gamma(1600.,400.)|True"],
#    "r_prior@hyperparams": ["dirichlet|1.|False", "dirichlet|uniform|True|0.1", "dirichlet_nn|uniform|True|0.01"]
#    }
#eval_codes = {
#    "ln": ["1000", "5000"],
#    "tx": ["16", "32", "64"],
#    "ks": ["k+m025", "k+m05", "k+m1", "k+m2", "k+m4"],
#    "rp": ["dir+1", "dir+uni+l", "dir+nn+l"]
#    }

evaluations = {
    "nb_sites@data": ["1000"],
    "nb_taxa@data": ["16"],
    "sim_kappa@data": ["gamma(6.25,25.)|True", "gamma(100.,100.)|True", "gamma(1600.,400.)|True"],
    "b_prior@hyperparams": ["exponential|10.|False", "exponential|uniform|True|0.1"],
    "r_prior@hyperparams": ["dirichlet|1.|False", "dirichlet|uniform|True|0.1"],
    "f_prior@hyperparams": ["dirichlet|1.|False", "dirichlet|uniform|True|0.1"]
    }

# [REQUIRED] Code names for evaluations
# It will be used to name jobs and output files
# It should have the same structure and item orders as
# evaluations option
eval_codes = {
    "ln": ["1000"],
    "tx": ["16"],
    "ks": ["k+m025", "k+m1", "k+m4"],
    "bp": ["exp+m01", "exp+uni+l"],
    "rp": ["dir+1", "dir+uni+l"],
    "fp": ["dir+1", "dir+uni+l"]
    }

# #############################################################
# nnTreeVB template configuration sections
# #############################################################

[io]
# output_path is used to save training data, scores and figures
# It will be updated to ../exp_outputs/<jobs_code>
# <jobs_code> is defined in write_run_rep_exps.py
output_path = to_be_updated_by_the_script

# To use your FASTA files (real data for example), put sim_data
# to False and specify the path of FASTA files
seq_file = to_be_updated_by_the_script
nwk_file = to_be_updated_by_the_script

# If False the program runs the evaluation and save resutls in
# output_path,else the program loads the results directly from
# the file (if it exists)
scores_from_file = False

# job_name will be updated automatically
job_name = to_be_updated_by_the_script

[data]
# If sim_data = True: simulate a tree and evolve new sequences
# else: fetch sequences from seq_file and nwk_file
sim_data = True

# nunmber of data replicates
nb_rep_data = 100

# If seq_from_files and sim_data are True, the data will be
# extracted from simulated FASTA files if they already exist,
# else: new alignment will be simulated
seq_from_file = False
# If nwk_from_file and sim_data are True, the tree and 
# branch lengths will be used to simulate the alignment
# (sim_blengths will not be used in this case)
nwk_from_file = False

# nb_sites is the size of simulated alignment
# The site patterns are not unique
nb_sites = 1000
# nb_taxa is the number of species (leaves of the tree)
nb_taxa = 32

# Substitution model to be used for sequence simulation
# jc69, k80, hky, gtr 
subs_model = hky

# Simulate a different tree for each data replicate (True)
# or simulate one tree for all data replicates (False, None)
sim_rep_trees = True

## Prior parameters for simulation
## ###############################
## IF sim_data is True, nntreevb.py will use the following 
## prior hyperparameters to simulate sequence alignments
## ELSE nntreevb.py will use them for comparison with
## its estimated parameters

## These priors are not used for inference. For inference
## prior check hyperparameters section

## Accepted distributions: (see torch_distributions.py module)
## normal, lognormal, gamma, dirichlet, categorical,
## exponential, uniform

# Branch lengths
# dist(param list)
# dist_ext(param list);dist_int(param list)

# SIM_REPS is a boolean to simulate the same set for all
# nb_rep_data replicates (False) or different set for
# each replicate (True)
# For exmple, if SIM_REPS of sim_blengths is False, all 
# trees will have the same set of branch lengths

# Examples:
## Same prior for all all branches
sim_blengths = exponential(10.)|True
## Two priors for external and internal branches resp.
#sim_blengths = uniform(0.01, 0.05);uniform(0.01, 0.02)

# Substitution rates (GTR)
#sim_rates = dirichlet(1)
# It wont be used here.
sim_rates = dirichlet(10.)|True
#            AG     AC     AT     GC     GT     CT
#sim_rates = 0.160, 0.160, 0.160, 0.160, 0.160, 0.160
#sim_rates = 0.225, 0.106, 0.164, 0.064, 0.070, 0.369
#sim_rates = 0.160

# Relative frequencies (HKY and GTR)
#sim_freqss = dirichlet(1.)
sim_freqs = dirichlet(10.)|True
#            A      G      C      T
#sim_freqs = 0.288, 0.190, 0.200, 0.326
# sim_freqs = 0.250

# Kappa (K80 and HKY)
# sim_kappa will be overridden by evaluations
sim_kappa = gamma(1., 1.)
#sim_kappa = 1.

# real_params is a flag to consider the parameters given
# in this section real or not. If they are, they will be used
# to compare with the estimated parameters by the variational
# model
real_params = True

[hyperparams]
# Hyper-parameters of the model set by user

# Substitution model to be used for variational model inference
# jc69 := infer b latent variables
# k80  := infer b, k latent variables
# hky  := infer b, k, f latent variables
# gtr  := infer b, r, f latent variables
subs_model = gtr

# Branches (All subs models)
#         distr. name|params of distr.|learn_prior[|lr]
b_prior = exponential|10.|False
#       distr. name|params|transform[|lr]
b_var = normal|0.,1.|lower_0|0.1

# Rates (GTR)
#         distr. name| params| learn_prior
# the params of dirichlet are initialized using a uniform dist
# r_prior will be overridden by evaluations
r_prior = dirichlet|1.|False
r_var = normal|0.,1.|simplex|0.1

# Frequencies (HKY and GTR)
f_prior = dirichlet|1.|False
f_var = normal|0.,1.|simplex|0.1

# Hyper parameters for (used) neural networks
h_dim = 32
nb_layers = 3
bias_layers = True
activ_layers = relu
dropout_layers = 0.

# fitting hyperparams
nb_rep_fit = 10
elbo_type = elbo
grad_samples = 1
K_grad_samples = 1
# A large nb of samples can consume a lot of memory
nb_samples = 1000
alpha_kl = 1.
max_iter = 2000
# optimizer type : adam | sgd
optimizer = adam
learning_rate = 0.1
weight_decay = 0.
save_fit_history = False
save_grad_stats = False
save_weight_stats = False

[settings]
# number of parallel replicates
nb_parallel = 4
# cpu | cuda | cuda:0 | mps
device = cpu
# dtype [float32 | float64]
dtype = float32
# Valid values for verbose: True, False, None and
# positive integers 
verbose = 1
compress_files = True

[plotting]
# To render Tex text in plots, Matplotlib requires
# Latex, dvipng, Ghostscript and type1ec.sty found in cm-super
# If Latex and other required packages are not installed,
# put to False
plt_usetex = False
size_font = 16
print_xtick_every = 400
logl_data=True
# ylimits for lineplots
#ylim_dists=-0.01,0.21
#ylim_scaled_dists=-0.01,0.21
#ylim_corrs=0.25,1.01
#ylim_ratios=-0.01,None
#
ylim_dists=-0.01,None
ylim_scaled_dists=-0.01,None
ylim_corrs=None,1.01
ylim_ratios=None,None
# legends
legend_elbos=best
legend_dists=upper right
legend_scaled_dists=upper right
legend_corrs=lower right
legend_ratios=best
