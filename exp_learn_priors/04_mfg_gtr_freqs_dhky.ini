## ############################################################
## Template config file for write_run_nntreevb_exps.py
## 
## Hyper-parameters and default values 
## ############################################################

## ############################################################
## nntreevb_write_run_exps.py configuration sections
## ############################################################

## [slurm] and [evaluation] sections will be removed in the 
## final config files for nntreevb.py

[slurm]
run_slurm = True
## if run_slurm=False: the script will launch the jobs locally
## elif run_slurm=True: it will launch the jobs on slurm

## ############################################################
## WARNING:
## In the case of run_slurm=False, the script will run
## several tasks locally in the background. Make sure that you
## have the necessary resources on your machine.
## It can crash the system.
## You can modify the grid of parameters to be evaluated here 
## to reduce the number of scenarios.
## ############################################################

## Account: UPDATE your slurm account information here
account = ctb-banire
#account = def-banire
mail_user = amine.m.remita@gmail.com

## SLURM parameters
## ################
# If gpus_per_node > 0, device in [settings] will be updated 
# to cuda
gpus_per_node = 0
cpus_per_task = 24
exec_time = 03:00:00
mem = 100000M
## For testing
#exec_time = 00:05:00
#mem = 8000M

[evaluation]
#run_jobs = False
run_jobs = True
## If run_jobs = False: the script generates the nntreevb
## config files but it won't launch the jobs.
## If run_jobs=True: it runs the jobs

##
output_eval = ../results/exp_learn_priors/

max_iter = 2000
## For testing
#max_iter = 500

nb_parallel = ${slurm:cpus_per_task}

# for nntreevb_sum_plot_exps.py
report_n_epochs = 2000

## Remark: values can span multiple lines, as long as they are
## indented deeper than the first line of the value.
## Configparser fetchs the value as string, after that it will
## be casted to python objects using json.loads

# I will use HKY model with different freqs values
# to simulate the data
# HKY model needs two params: kappa and frequencies
# For kappa, I will use gamma(100.,100.)|True
# For branch lengths, I use exponential(10.)|True
# For frequencies:

# Original batch of evaluations
evaluations = {
    "nb_sites@data": ["1000"],
    "nb_taxa@data": ["64"],
    "sim_freqs@data": ["dirichlet(10.,1.,1.,10.)|True", "dirichlet(10.)|True", "dirichlet(1.,10.,10.,1.)|True"],
    "b_prior@hyperparams": ["exponential|10.|False", "exponential|uniform|True|0.1", "exponential_nn|uniform|True|0.01"],
    "r_prior@hyperparams": ["dirichlet|1.|False", "dirichlet|uniform|True|0.1", "dirichlet_nn|uniform|True|0.01"],
    "f_prior@hyperparams": ["dirichlet|1.|False", "dirichlet|uniform|True|0.1", "dirichlet_nn|uniform|True|0.01"]
    }

eval_codes = {
    "ln": ["1000"],
    "tx": ["64"],
    "fs": ["dir+10AT", "dir+10", "dir+10GC"],
    "bp": ["exp+m01", "exp+uni+l", "exp+nn+l"],
    "rp": ["dir+1", "dir+uni+l", "dir+nn+l"],
    "fp": ["dir+1", "dir+uni+l", "dir+nn+l"]
    }

#evaluations = {
#    "nb_sites@data": ["1000"],
#    "nb_taxa@data": ["64"],
#    "sim_freqs@data": ["dirichlet(10.,1.,1.,10.)|True", "dirichlet(10.)|True", "dirichlet(1.,10.,10.,1.)|True"],
#    "b_prior@hyperparams": ["exponential_nn|uniform|True|0.01"],
#    "r_prior@hyperparams": ["dirichlet_nn|uniform|True|0.01"],
#    "f_prior@hyperparams": ["dirichlet_nn|uniform|True|0.01"]
#    }
#
## [REQUIRED] Code names for evaluations
## It will be used to name jobs and output files
## It should have the same structure and item orders as
## evaluations option
#eval_codes = {
#    "ln": ["1000"],
#    "tx": ["64"],
#    "fs": ["dir+10AT", "dir+10", "dir+10GC"],
#    "bp": ["exp+nn+l"],
#    "rp": ["dir+nn+l"],
#    "fp": ["dir+nn+l"]
#    }

# #############################################################
# nnTreeVB template configuration sections
# #############################################################

[io]
# output_path is used to save training data, scores and figures
# It will be updated to ../exp_outputs/<jobs_code>
# <jobs_code> is defined in write_run_rep_exps.py
output_path = to_be_updated_by_the_script

# To use your FASTA files (real data for example), put sim_data
# to False and specify the path of FASTA files
seq_file = to_be_updated_by_the_script
nwk_file = to_be_updated_by_the_script

# If False the program runs the evaluation and save resutls in
# output_path,else the program loads the results directly from
# the file (if it exists)
scores_from_file = False

# job_name will be updated automatically
job_name = to_be_updated_by_the_script

[data]
# If sim_data = True: simulate a tree and evolve new sequences
# else: fetch sequences from seq_file and nwk_file
sim_data = True

# nunmber of data replicates
nb_rep_data = 100

# If seq_from_files and sim_data are True, the data will be
# extracted from simulated FASTA files if they already exist,
# else: new alignment will be simulated
seq_from_file = False
# If nwk_from_file and sim_data are True, the tree and 
# branch lengths will be used to simulate the alignment
# (sim_blengths will not be used in this case)
nwk_from_file = False

# nb_sites is the size of simulated alignment
# The site patterns are not unique
nb_sites = 1000
# nb_taxa is the number of species (leaves of the tree)
nb_taxa = 32

# Substitution model to be used for sequence simulation
# jc69, k80, hky, gtr 
subs_model = hky

# Simulate a different tree for each data replicate (True)
# or simulate one tree for all data replicates (False, None)
sim_rep_trees = True

## Prior parameters for simulation
## ###############################
## IF sim_data is True, nntreevb.py will use the following 
## prior hyperparameters to simulate sequence alignments
## ELSE nntreevb.py will use them for comparison with
## its estimated parameters

## These priors are not used for inference. For inference
## prior check hyperparameters section

## Accepted distributions: (see torch_distributions.py module)
## normal, lognormal, gamma, dirichlet, categorical,
## exponential, uniform

# Branch lengths
# dist(param list)
# dist_ext(param list);dist_int(param list)

# SIM_REPS is a boolean to simulate the same set for all
# nb_rep_data replicates (False) or different set for
# each replicate (True)
# For exmple, if SIM_REPS of sim_blengths is False, all 
# trees will have the same set of branch lengths

# Examples:
## Same prior for all all branches
sim_blengths = exponential(10.)|True
## Two priors for external and internal branches resp.
#sim_blengths = uniform(0.01, 0.05);uniform(0.01, 0.02)

# Substitution rates (GTR)
# AG     AC     AT     GC     GT     CT
#sim_rates = dirichlet(1)
# It wont be used here.
sim_rates = dirichlet(10.)|True

# Relative frequencies (HKY and GTR)
# A      G      C      T
# sim_freqs will be overridden by evaluations
sim_freqs = dirichlet(10.)|True

# Kappa (K80 and HKY)
sim_kappa = gamma(100.,100.)|True

# real_params is a flag to consider the parameters given
# in this section real or not. If they are, they will be used
# to compare with the estimated parameters by the variational
# model
real_params = True

[hyperparams]
# Hyper-parameters of the model set by user

# Substitution model to be used for variational model inference
# jc69 := infer b latent variables
# k80  := infer b, k latent variables
# hky  := infer b, k, f latent variables
# gtr  := infer b, r, f latent variables
subs_model = gtr

# Branches (All subs models)
#         distr. name|params of distr.|learn_prior[|lr]
b_prior = exponential|10.|False
#       distr. name|params|transform[|lr]
b_var = normal|0.,1.|lower_0|0.1

# Rates (GTR)
#         distr. name| params| learn_prior
# the params of dirichlet are initialized using a uniform dist
r_prior = dirichlet|1.|False
r_var = normal|0.,1.|simplex|0.1

# Frequencies (HKY and GTR)
# f_prior will be overridden by evaluations
f_prior = dirichlet|1.|False
f_var = normal|0.,1.|simplex|0.1

# Hyper parameters for (used) neural networks
h_dim = 32
nb_layers = 3
bias_layers = True
activ_layers = relu
dropout_layers = 0.

# fitting hyperparams
nb_rep_fit = 10
elbo_type = elbo
grad_samples = 1
K_grad_samples = 1
# A large nb of samples can consume a lot of memory
nb_samples = 1000
alpha_kl = 1.
max_iter = 2000
# optimizer type : adam | sgd
optimizer = adam
learning_rate = 0.1
weight_decay = 0.
save_fit_history = False
save_grad_stats = False
save_weight_stats = False

[settings]
# number of parallel replicates
nb_parallel = 4
# cpu | cuda | cuda:0 | mps
device = cpu
# dtype [float32 | float64]
dtype = float32
# Valid values for verbose: True, False, None and
# positive integers 
verbose = 1
compress_files = True

[plotting]
# To render Tex text in plots, Matplotlib requires
# Latex, dvipng, Ghostscript and type1ec.sty found in cm-super
# If Latex and other required packages are not installed,
# put to False
plt_usetex = False
size_font = 16
print_xtick_every = 400
logl_data=True
# ylimits for lineplots
#ylim_dists=-0.01,0.21
#ylim_scaled_dists=-0.01,0.21
#ylim_corrs=0.25,1.01
#ylim_ratios=-0.01,None
#
ylim_dists=-0.01,None
ylim_scaled_dists=-0.01,None
ylim_corrs=None,1.01
ylim_ratios=None,None
# legends
legend_elbos=best
legend_dists=upper right
legend_scaled_dists=upper right
legend_corrs=lower right
legend_ratios=best
