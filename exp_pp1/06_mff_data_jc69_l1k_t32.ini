## ############################################################
## Template config file for write_run_nntreevb_exps.py
## 
## Hyper-parameters and default values 
## ############################################################

## ############################################################
## nntreevb_write_run_exps.py configuration sections
## ############################################################

## [slurm] and [evaluation] sections will be removed in the 
## final config files for nntreevb.py

[slurm]
run_slurm = True
## if run_slurm=False: the script will launch the jobs locally
## elif run_slurm=True: it will launch the jobs on slurm

## ############################################################
## WARNING:
## In the case of run_slurm=False, the script will run
## several tasks locally in the background. Make sure that you
## have the necessary resources on your machine.
## It can crash the system.
## You can modify the grid of parameters to be evaluated here 
## to reduce the number of scenarios.
## ############################################################

## Account: UPDATE your slurm account information here
account = ctb-banire
#account = def-banire
mail_user = amine.m.remita@gmail.com

## SLURM parameters
## ################
# If gpus_per_node > 0, device in [settings] will be updated 
# to cuda
gpus_per_node = 0
cpus_per_task = 24
exec_time = 05:00:00
mem = 64000M
## For testing
#exec_time = 00:05:00
#mem = 8000M

[evaluation]
#run_jobs = False
run_jobs = True
## If run_jobs = False: the script generates the nntreevb
## config files but it won't launch the jobs.
## If run_jobs=True: it runs the jobs

##
output_eval = ../exps/

max_iter = 2000
## For testing
#max_iter = 500

nb_parallel = ${slurm:cpus_per_task}

# for nntreevb_sum_plot_exps.py
report_n_epochs = 2000

## Remark: values can span multiple lines, as long as they are
## indented deeper than the first line of the value.
## Configparser fetchs the value as string, after that it will
## be casted to python objects using json.loads

evaluations = {
    "sim_blengths@data": ["gamma(0.001,1.)|True", "gamma(0.025,5.)|True", "gamma(0.1,10.)|True", "gamma(2.5,50.)|True", "gamma(10.,100.)|True", "gamma(250.,500.)|True"],
    "b_prior@hyperparams": ["exponential|10.|False", "exponential_nn|uniform|True|0.01", "gamma_nn|uniform|True|0.01"],
    "b_var@hyperparams": ["gamma|1.,10.|False|0.1", "gamma|uniform|False|0.01", "gamma_nn|uniform|False|0.001"]
    }

# [REQUIRED] Code names for evaluations
# It will be used to name jobs and output files
# It should have the same structure and item orders as
# evaluations option
eval_codes = {
    "bd": ["g+m0001v0001", "g+m0005v0001", "g+m001v0001", "g+m005v0001", "g+m01v0001", "g+m05v0001"],
    "bp": ["exp01", "exp+nn+l", "gamma+nn+l"],
    "bv": ["gamma+m01v001", "gamma+unif", "gamma+nn"]
    }

# #############################################################
# nnTreeVB template configuration sections
# #############################################################

[io]
# output_path is used to save training data, scores and figures
# It will be updated to ../exp_outputs/<jobs_code>
# <jobs_code> is defined in write_run_rep_exps.py
output_path = to_be_updated_by_the_script

# To use your FASTA files (real data for example), put sim_data
# to False and specify the path of FASTA files
seq_file = to_be_updated_by_the_script
nwk_file = to_be_updated_by_the_script

# If False the program runs the evaluation and save resutls in
# output_path,else the program loads the results directly from
# the file (if it exists)
scores_from_file = False

# job_name will be updated automatically
job_name = to_be_updated_by_the_script

[data]
# If sim_data = True: simulate a tree and evolve new sequences
# else: fetch sequences from seq_file and nwk_file
sim_data = True

# nunmber of data replicates
nb_rep_data = 100

# If seq_from_files and sim_data are True, the data will be
# extracted from simulated FASTA files if they already exist,
# else: new alignment will be simulated
seq_from_file = False
# If nwk_from_file and sim_data are True, the tree and 
# branch lengths will be used to simulate the alignment
# (sim_blengths will not be used in this case)
nwk_from_file = False

# nb_sites is the size of simulated alignment
# The site patterns are not unique
nb_sites = 1000
# nb_taxa is the number of species (leaves of the tree)
nb_taxa = 32

# Substitution model to be used for sequence simulation
# jc69, k80, hky, gtr 
subs_model = jc69

# Simulate a different tree for each data replicate (True)
# or simulate one tree for all data replicates (False, None)
sim_rep_trees = True

## Prior parameters for simulation
## ###############################
## IF sim_data is True, nntreevb.py will use the following 
## prior hyperparameters to simulate sequence alignments
## ELSE nntreevb.py will use them for comparison with
## its estimated parameters

## These priors are not used for inference. For inference
## prior check hyperparameters section

## Accepted distributions: (see torch_distributions.py module)
## normal, lognormal, gamma, dirichlet, categorical,
## exponential, uniform

# Branch lengths
# dist(param list)
# dist_ext(param list);dist_int(param list)

# SIM_REPS is a boolean to simulate the same set for all
# nb_rep_data replicates (False) or different set for
# each replicate (True)
# For exmple, if SIM_REPS of sim_blengths is False, all 
# trees will have the same set of branch lengths

# Examples:
## Same prior for all all branches
sim_blengths = exponential(10.)|True
## Two priors for external and internal branches resp.
#sim_blengths = uniform(0.01, 0.05);uniform(0.01, 0.02)

# real_params is a flag to consider the parameters given
# in this section real or not. If they are, they will be used
# to compare with the estimated parameters by the variational
# model
real_params = True

[hyperparams]
# Hyper-parameters of the model set by user

# Substitution model to be used for model inference
# jc69 := infer b latent variables
# k80  := infer b, k latent variables
# hky  := infer b, k, f latent variables
# gtr  := infer b, r, f latent variables
subs_model = jc69

# Branches (All subs models)
#         distr. name|params of distr.|learn_prior[|lr]
b_prior = exponential|10.|False

#       distr. name|params|transform[|lr]
b_var = normal|0.1,0.1|lower_0

# Hyper parameters for (used) neural networks
h_dim = 32
nb_layers = 3
bias_layers = True
activ_layers = relu
dropout_layers = 0.

# fitting hyperparams
nb_rep_fit = 10
elbo_type = elbo
grad_samples = 1
K_grad_samples = 1
# A large nb of samples can consume a lot of memory
nb_samples = 1000
alpha_kl = 1.
max_iter = 2000
# optimizer type : adam | sgd
optimizer=adam
learning_rate = 0.1
weight_decay = 0.
save_fit_history = False
save_grad_stats = False
save_weight_stats = False

[settings]
# number of parallel replicates
nb_parallel = 4
# cpu | cuda | cuda:0 | mps
device = cpu
# dtype [float32 | float64]
dtype = float32
# Valid values for verbose: True, False, None and
# positive integers 
verbose = 1
compress_files = True

[plotting]
# To render Tex text in plots, Matplotlib requires
# Latex, dvipng, Ghostscript and type1ec.sty found in cm-super
# If Latex and other required packages are not installed,
# put to False
plt_usetex = False
size_font = 16
print_xtick_every = 400
logl_data=True
